<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|EB Garamond:300,300italic,400,400italic,700,700italic|Source Code Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wadmes.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"buttons","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta name="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
<meta property="og:type" content="website">
<meta property="og:title" content="NEWAY">
<meta property="og:url" content="http://wadmes.github.io/page/6/index.html">
<meta property="og:site_name" content="NEWAY">
<meta property="og:description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Li Wei">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://wadmes.github.io/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>NEWAY - 愿识乾坤大，仍怜草木青</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">NEWAY</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">愿识乾坤大，仍怜草木青</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-reading-notes">

    <a href="/notes" rel="section"><i class="fas fa-book-reader fa-fw"></i>Reading notes</a>

  </li>
        <li class="menu-item menu-item-about-me">

    <a href="/about" rel="section"><i class="fas fa-user fa-fw"></i>About me</a>

  </li>
        <li class="menu-item menu-item-official-about-me">

    <a href="/cv" rel="section"><i class="fas fa-user-secret fa-fw"></i>Official About me</a>

  </li>
        <li class="menu-item menu-item-resume">

    <a href="/cv/cv_liwei/cv.pdf" rel="section"><i class="fas fa-file fa-fw"></i>Resume</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/10/13/%E8%AE%B0%E9%AB%98%E6%AD%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/13/%E8%AE%B0%E9%AB%98%E6%AD%A6/" class="post-title-link" itemprop="url">记高武</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-13 21:36:34 / Modified: 10:53:50" itemprop="dateCreated datePublished" datetime="2020-10-13T21:36:34-04:00">2020-10-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/10/13/%E8%AE%B0%E9%AB%98%E6%AD%A6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/10/13/%E8%AE%B0%E9%AB%98%E6%AD%A6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>然后到了我的管家，高武了。他是高二来到我们班的，一开始他孤僻的性格加上奇葩的声音使他看起来很不合群，还经常被我们201耻笑…后来他坐到了我后面，可能由于我看他老实吧，他成了我管家，帮我打理琐碎事务，说来惭愧，作为我的管家我愣是没给过他工资。（后来玩开后笑萍一直拿这个怂恿高武，搞得后来他越来越不老实了。）高武是个胸有大志的人，但他的志向从来不会告诉别人，我还是在他的本子上看到密密麻麻的笔记才知道的，因为那笔记只包含了四个字——武定中原…霸气！同样的，他也不会说出自己的心事，我知道他有心事——他的家庭，他的前途，他的成绩，但他一概不提，他总是一个人憋在心里，他会午睡时一个人孤独的成四十五度角仰望窗台外明朗的天空和远方如中国画一般墨绿的山峰，他会睡觉时翻来覆去思考与他有关的一切事，他会在他许多很“非主流”的本子上记下他所思所想，这也许是他的思考方式吧。</p>
<p>额，在这里研究他的心理似乎有点不厚道，不说这方面了。</p>
<p>高武不善言辞，很多时候都是默默的承受，还是我最压抑的日子，他从没鼓励我什么的，但高考完，小姨问起我我们班有没有人名字里有个武，那时我才知道，原来高武一直发短信给我小姨，让我小姨来安慰我。听完我内心诧异了许久，原来那个呆呆木木的高武也会关心人诶，原来世界上还有许多人挂念，关心着我，这种感觉真是别样的幸福。</p>
</blockquote>
<p>这段是我高中刚毕业时写下的和高武有关的回忆。我觉得我有一些需要补充的必要。我第一次和高武的接触应该是由此在201讨论他的时候：应该是说他奇怪的声音和某些特点，譬如止不住的咳嗽？总之睡在宿舍门口上铺的我看到了高武正在偷听：那个瘦弱矮小性格冷淡的男生正侧着身子将耳朵放到门边听着我们对他的讨论，我已忘记我们是否带着嘲讽意味，但愿没有。</p>
<p>后来他坐我后面，具体怎么成了前后桌的我也忘了，不过很高兴的是我并没有对这个有点“被排挤”的同学有一点不满或是不乐意接触的意思。记得班上有过另外一个新来的同学也是经常咳嗽，结果她前排的那位同学对此很不满，总是觉得她咳口水到自己的头发上了，这方面我倒是无所谓，毕竟我自己就很邋遢。</p>
<p>总之成了前后桌之后我和他的关系直线上升，直至经常一起吃饭。我比较邋遢，爸妈给的钱和一些零钱也总是扔在一起，见高武整理的井井有条，便索性委托高武帮我管理财物。还称之为管家。甚至都达成了少年之约，待我功成名就需要管家的时候，一定请高武来当我家总管。学习上，虽然我偶有教高武一些我的学习方法，不过他也不怎么问我题目，可能和很多同学一样，不想因为问题太简单被我耻笑吧。</p>
<p>高中毕业后，高武也曾联系我，问我“李巍，你现在过的怎么样？”。我过了很久才看到消息，回到“很好啊，高武，你呢？”。然后便没有下文了。这也是我最追悔莫及的一点，倘若我早点回复，或者跟高武继续聊天保持联系，至少我还能在他身前给他更多的一点温暖。</p>
<p>这是我能补充的所有片段了，我再怎么想，也没有一个向后的时光机可以倒转。不过再翻看上面的回忆，这个天然前进的时光机倒是让有些问题在现在得到了答案，譬如他的家庭、他的前途、他的心事，我总算了解了一些，虽然为时已晚。高武本身家庭就很贫困，他家在一个离镇上都还有接近两小时车程的山沟沟里，我问了当地村长，还好那个地方和镇上每天都有班车来回，不过即便如此高武往返学校来回都需要至少十小时的车程。出发前我其实对找到高武的住处也有着一丝担忧，我只知道他是哪个村的，其他一概不知，万幸我在乡卫生所，其实也就是一个很小的医药门面，碰到了他们村的村长，在村长的指引下，我找到了他的住处。他父亲当时正在收割稻田，村长下田里叫他，叔叔便赤着脚走回了家里，脚上还沾满了水田里的泥土。高武是在过年的时候在他姑父家出的意外，当时他帮姑父一家拍照一直往后退，便不慎跌下楼。骨灰也留在了姑父家，没有带回来。他的妈妈因为吃农药过世了，我不清楚是高武去世前还是之后，但总之他父亲现在一个人孤苦伶仃的生活着。我随着叔叔进了门，看着基本家徒四壁的光景，想到我本应该很早就来他家里看看，我眼泪又止不住的流。但是我没有像第一次知道消息时一直痛哭，我内心告诉自己不能再流泪了，高武的爸爸才是最伤心的那个人，我不能再勾起他更多的痛苦的回忆。我将买的水果牛奶给了叔叔，但是他硬是拒绝了我给的钱，无奈，我只能加上叔叔的微信，告别了高武的故乡。</p>
<blockquote>
<p>说到高中，自从知道了那件事后，很不想再和过去的时光有什么瓜葛：有点我以我心对明月，奈何明月照沟渠的意思。但是对于高武，确实是让我感动到耿耿于怀的：那个在草稿本上写满武定中原，看起来深不可测的高武，高考前两个月一直和我小姨发短信说我压力很大让小姨安慰安慰我。高中刚毕业时，对于这些，粗线条的我只是感动，然后一带而过。但现在越发觉得，要做到这一步，实在是太无私伟大了。人生在世，大多数人只是为了自己：和你相处是为了自己的前途或是为了自己能开心，倘若你对我的前途无用、或是你惹的我不开心了，我屁股一拍就可以溜了，临走时还可以忿忿道对方如何如何不好。而像高武，便是我钦佩和想成为的，温暖而不张扬，照顾他人情绪而不仅仅是为了自己。可惜的是高中后便和他断了联系，纵使一直qq找他也了无音信。</p>
</blockquote>
<p>上面这段，是今年四月份我碎碎念时念叨的关于高武的片段。当时我还不知道高武的噩耗，我只当高武和我无数同学朋友一样，只是有了各自的生活所以少了联系，但当再次遇见时，便又能相视大笑，共忆少年时。</p>
<p>直到我再赶ICCAD的前一两个星期，无意中得知了他的意外。一瞬间我的双脚像没了知觉一样瘫倒在地，我倒在地上嚎啕大哭，脑子里一片空白，只有难过。我哭了不知多久，把自己眼泪哭干了，力气哭没了，肚子哭饿了，便去找水喝，可是水也拿不稳，我便继续倚在打水的台子上哭。</p>
<p>距离高武离开已经快六年了，距离我知道这个消息也半年了，但终究仍然莫名其妙有一些时刻，那种难以置信、悲痛欲绝的感觉会涌上我的心头。我是个缺爱的人，也因此我很珍惜那些温柔的、会表达爱的、无条件对我好的人。可惜当我认识到自我、认识到哪些人是这样的人的时候已经晚了。不过按照活在心中便不是死亡的概念，高武一直不会离去，我也要带着他的温柔，和他一样，做一个体贴、无私、善良的人。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/10/03/after-ICLR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/03/after-ICLR/" class="post-title-link" itemprop="url">After ICLR 2021</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-03 20:17:18" itemprop="dateCreated datePublished" datetime="2020-10-03T20:17:18-04:00">2020-10-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-18 12:48:59" itemprop="dateModified" datetime="2024-01-18T12:48:59-05:00">2024-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/10/03/after-ICLR/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/10/03/after-ICLR/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          Forbidden area!
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/10/03/after-ICLR/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/07/20/Hey-Kong%EF%BC%8CJuly%EF%BC%8C2020/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/20/Hey-Kong%EF%BC%8CJuly%EF%BC%8C2020/" class="post-title-link" itemprop="url">Hey Kong，July，2020</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-20 02:16:29" itemprop="dateCreated datePublished" datetime="2020-07-20T02:16:29-04:00">2020-07-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-19 14:53:06" itemprop="dateModified" datetime="2020-07-19T14:53:06-04:00">2020-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/07/20/Hey-Kong%EF%BC%8CJuly%EF%BC%8C2020/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/07/20/Hey-Kong%EF%BC%8CJuly%EF%BC%8C2020/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hey，Neway。这里是2020年炎热的夏天。</p>
<p>2020年可能是你这辈子一提起都会感慨一些事的一年：噢那年啊，那个新冠可是持续了一年，我戴了一年口罩，在宿舍办公了一年；噢那年中美彻底撕破脸了；对我来说，除了这些国家大事，自己的人生也是在一个重大的转折点上，这也是我为什么想给你写封信，而不是按之前那样，碎碎念的记录日子：不知道这个转折点后的你，会在哪里，带着什么样的心情读这封信，所以也是蛮有意思的。</p>
<p>其实你也知道这个转折点是什么意思：今年是准备申请的一年，现在是7月20日，我也背了快2&#x2F;3的单词了，但是每次复习的时候还是有很多单词不认识。教授也还没开始套瓷，感觉教授们的方向没有一个和我特别match，做GCN的没有做EDA的，做EDA的没有做GCN的。没套瓷还有另一个原因：ICCAD没中啦。</p>
<p>感觉写的像野马奔腾一样没有主线，那就先说说ICCAD没中这件事上。给个评价的话，算是意料之中但又出乎意料吧，意料之中是因为实验都没做完，test layout只拿了一个circuit，其他的实在是跑不完了；就这一点就足够杀掉这篇paper了。意料之外是这个work比起DAC那篇是让我满意特别多的work；是一个让我觉得自己不是一个只知道套DL SOTA的缝合怪的work；也是一个可以让我特别自豪的showoff的work。哎，所以知道没中的时候真的还是感觉好可惜呐，本来还想着iccad也直接中，就可以底气十足的抬头挺胸去套瓷了，但现在就很尴尬了，申请前我能投的paper除去这篇recycle到ASPDAC的就只有一篇target ICLR的work了，而且也没有recycle的机会了。。。哎，只能安慰自己publication并不算很重要吧。</p>
<p>除去对我申请的直接影响，这次没中还让我给自己一个清晰的认知：或者说是泼了一盆冷水，虽然自诩想当威斯布鲁克不想当科沃尔，但是真没中需要recycle的时候还是让我明白，我不是什么特别适合research的天才。基于此，有个最近困扰我很久的更严重的问题：所谓德不配位，我这种水平的臭弟弟，就算侥幸伸到了四大的PHD，进去后真能做出什么影响整个圈子的work吗？哎，每每想到这，心情就沉重了三分，如人饮水，冷暖自知，自己的水平是怎样的，只有自己最清楚，就像初高中随随便便就能考第一，大一大二翘一学年课都能拿A，是因为知道自己就是比起其他人强，但是真的到了PHD要搞research到时候，也是能清楚感觉到自己读的书太少！”书到用时方恨少“哪！高三可以几个月把高一高二三国杀+奥赛落下的吃完，但是真的面对人类边界的那些数学知识，我也实在只是一个普通人哪，尤其是最近读GCN相关的理论paper，那些证明只能让我感慨妙哉，让我来写我却只能摸摸脑壳。英雄联盟有个玩笑话:</p>
<blockquote>
<p>不是你的分段，你不要碰</p>
</blockquote>
<p>放到研究里，不也有</p>
<blockquote>
<p>不是你脑子能搞懂的领域，你不要碰</p>
</blockquote>
<p>我很自信我的水平是当得起香港中文大学博士的，但是，譬如让我成为一个MIT的博士，我只能呵呵了。这就是别人常说的，欲戴皇冠，必承其重吧。</p>
<p>但是我终究还是要努力去申请的。这也是这段时间想通的。关于随遇而安和尽吾志也的关系。我一直都是一个随遇而安的人，也因此对很多东西没什么追求动力。让我转变想法的是关于高中的一段话，大意就是高中是最美好的时光，因为它代表着无限的可能，代表着无限的选择。随着开始准备申请，我意识到，自己的学习能力是在逐渐下降的，而这种下降，也意味着自己未来选择的权利和机会是越来越少的。同时，作为一个master，而不是PhD student，我还有也是仅有申请PhD这最后一个机会，来提高自己的可能性。所以，在学习能力还在巅峰的最后几年，去努力成为自己想成为的人，才不至于让自己在没有这个学习能力，没有这个申请机会的时候叹息后悔。这才是“尽吾志也而不能至者，可以无悔矣”的真义哪，先有尽吾志也，才有无悔。哈哈，原谅我这个年轻人对于人生观的多变，不知道2030年的你看到这些会是怎样的想法，又会有怎样的人生观，这也算是我们生而为人很奇妙的一件事吧。</p>
<p>啊，其实还有很多sub-topic想说，但是已经三点了，我实在是得睡了，所以下次再说吧。最近总是中午才起床，拖拖拉拉就是两三点才开始办公，事情都拖着，每天还要两小时背单词，之后一定得最大化利用时间了！</p>
<p>Fighting!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/" class="post-title-link" itemprop="url">超精简的GNN综述</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-27 01:01:52" itemprop="dateCreated datePublished" datetime="2020-06-27T01:01:52-04:00">2020-06-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-01 19:05:43" itemprop="dateModified" datetime="2022-05-01T19:05:43-04:00">2022-05-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>前言。本文作于GCN横空出世（2017）的三年后（2020），想尽可能在简单易懂的情况下说清楚近年GNN的发展脉络。当然，凭我鸽子王的尿性，估计本来也就不会写的很翔实（简单是第一层，偷懒是第二层🐶）。</p>
<p>4th, March, 2021. Update: After a series of rejections by AI conferences, I note that I may look back on these papers and rethink why these papers can be accepted, i.e., review them in a positive and admired way instead of criticising them. Also, I try to pay attentions to the authors and institutes, and summarize the research style of different groups. This helps to understand the whole GNN community and “may” contribute to my PhD dicision.</p>
</blockquote>
<p>最近GNN又看了一些ICLR2020的paper，加之之前积累的paper，对整个GNN的发展脉络有了比较清楚的理解，之所以想要记录在案，是因为发现：</p>
<ul>
<li>GNN有点类似CNN，是一个入门难度不高，效果又比较出色的技术。不像EDA里面各种子问题及现在的SOTA解决方案，要解释清楚就要半天。</li>
<li>现在的各种文章因为作者们的严谨及数学功底的雄厚，导致涉及内容过多，我要真正看明白的阈值太高（对不起实在是因为我太菜了），因此心里琢磨着要是能有一篇浅显易懂的综述，快捷的理清paper间的区别和发展脉络该多好。</li>
<li>现在许多GNN的paper其实真正technique部分的contribution很容易概括，当然所谓的research也不可能是一蹴而就 novelty、results什么的全都完爆previous work。只是很多paper的出发点都不一样，导致看起来paper间差异很大，其实在我看来现在GNN还是一个比较混沌的状态，等着哪位X kaiming大神来盘古开天。（或者说GCN这篇就算是开天了？）</li>
</ul>
<p><strong>Note：这篇文章完全是为我本人量身打造，是为了未来的我复习GNN的时候有个参考。因此我面向的读者是有了比较充足的CNN、Graph知识背景的。</strong></p>
<h1 id="开山老祖-GCN"><a href="#开山老祖-GCN" class="headerlink" title="开山老祖 GCN"></a>开山老祖 GCN</h1><blockquote>
<p>这一段大部分是我的臆测。。可以直接看结论。</p>
</blockquote>
<p>给一个irregular的grpah&#x2F;point cloud，我们可以从频域(specture)和空间(spatial)角度去解决这个问题</p>
<ul>
<li>频域角度就是图信号映射到频域后再做卷积操作。（直接copy from zhihu，原谅我这一块其实并不是很清楚，因为现在的大部分work都不会提specture这个东西了。。。而且本身这个对数学功底要求就有点高，也许我真的phd了可以花时间研究这个？单就目前纯水paper的角度，这个建议不要碰，1.上手难，2.现在大部分SOTA都是spatial）</li>
<li>空间角度大概就是直接从点&#x2F;线的关系入手，用这些关系来表示某种message passing。messgae passing的意思很简单，就是说给一个点，它会把它自己的信息（feature）传输给它的邻点，同样，它的邻点也会把邻点的信息传给它。大概有点像社会学里面的信息传输，譬如李巍长得很帅，一传十十传百所有人都知道我长得很帅这样。。（我长得很帅这个信息就被所有人收到了，这样在他们那的信息就是：我有个朋友&#x2F;朋友的朋友长的很帅）</li>
</ul>
<p>这篇GCN做的就是，从频域角度出发给了GCN的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%5E%7B(l+1)%7D+=+%5Csigma+(%5Cwidetilde+D+%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+%5Cwidetilde+A+%5Cwidetilde+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+H%5E%7B(l)%7DW%5E%7B(l)%7D+)+%5C%5C+%5C%5C" alt="[公式]"></p>
<p>$H^l$就是第l层的所有点的feature，$\bar{A}$是加上identity matrix的矩阵，D是degree matrix为了normalization（结果其实比较接近mean），W就是映射到另一个空间增加信息量。</p>
<p>然后发现这个公式其实可以从空间的角度很elegant的解释，GCN的每一层其实就是aggragation + combing（encoding）</p>
<ul>
<li>$\bar{A}H$ 其实就是对于任何一个点，将自己的feature，以及邻点的feature 累加起来，加个D就是为了normalization（$D^{-1}$ 其实就是mean），因为每次都累加层数多了这个值不爆炸了？而且有些点邻居本来就多怎么办？。</li>
<li>$W^{l}$ 就是做一次combing啦，可以理解成投射到另一个空间增加复杂度（其实就是1*1conv，or FC，or MLP，哦，有的可能还会加个bias）</li>
</ul>
<h1 id="方法层面"><a href="#方法层面" class="headerlink" title="方法层面"></a>方法层面</h1><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p>这篇就直接不管specture了，直接从空间角度把GCN重新提了一遍，同时formally定义了aggregator这个概念，其实就是对于每个点，怎么将它自己及它邻点的信息聚合（aggregate）起来。然后还提出了几个方法。</p>
<ul>
<li>不加自己的feature mean 。</li>
<li>加上自己的feature mean。。。</li>
<li>LSTM。。。也许那个年代LSTM比较火吧。</li>
<li>channel-wise的max sampling</li>
</ul>
<p>注：aggregator其实总之不管是什么，都只要保证order-invarienty以及size-insensitivity就行了</p>
<h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><p>这篇就是说attention效果这么好，我们也套用到GCN吧！（对不起这是我的小人之心🐶），实际上应该是：每个邻居可能重要性不一样，所以要加个attention！</p>
<p><img src="https://www.zhihu.com/equation?tex=a_%7Blearn%7D(i,j)=%5Cfrac%7Bexp(LeakyReLU(w%5BWx_i,Wx_j%5D))%7D%7B%5CSigma_%7Bk%5Cin+neigh(i)%7Dexp(LeakyReLU(w%5BWx_i,Wx_k%5D))%7D%5C%5C" alt="[公式]"></p>
<p>a(i,j)就是点i对点j的重要性。但这个真把我秀到了，具体可以参见<a href="https://wadmes.github.io/2019/12/30/attention/">https://wadmes.github.io/2019/12/30/attention/</a></p>
<h2 id="JKNet"><a href="#JKNet" class="headerlink" title="JKNet"></a>JKNet</h2><p>这篇就是说DenseNet效果这么好，我们也套用到GCN吧！（其实background还是GCN层数过深，例如depth&gt;2，就会出现oversmooth的情况，即所有离得近点的feature长的都挺像的。而离得远的点互相的connection又太小了，所以加个densenet就可以让他们connection不小了呀呀呀呀呀呀）</p>
<blockquote>
<p>提到oversmoothing了就说下，其实直接原因是目前GCN的aggregation必须保证order invarienty，导致不像CNN里面的convolution kernel一样卷积时候可以带weight，想象一下CNN里面a pixel的右边是b pixel，传信息的时候用的是3*3kernel里面的(2,3)value，而b要传给a，就是(2,1)了，两个值不一样，所以不管传多少次都不会最后比较接近，除非(2,3) &#x3D; (1,3)，这就是GCN现在的情况，因为GCN不可能还有方向这个概念，或者说，给一个点a，他有邻居b，c，目前还不可能能分出b，c。。</p>
</blockquote>
<h2 id="DeepGCN"><a href="#DeepGCN" class="headerlink" title="DeepGCN"></a>DeepGCN</h2><p>这篇就是说ResNet效果这么好，我们也套用到GCN吧！</p>
<p>然后除了简单的加个residual connection，还提出了一个GCN上的dialated convolution，个人觉得是比residual 更重要&#x2F;有效的一点。大概就是：regular CNN里面dialated convolution可以很轻松的跳一个选一个这样卷积，但是GCN不好操作因为每个点邻居个数都不一样，而且还是无序的。这里的做法就是对于每个当前点，给其他点按照feature的欧几里得距离来个排序，然后就可以跳一个选一个啦！（其实有点做point cloud里面dynamic graph CNN那味了）</p>
<h2 id="Dropedge"><a href="#Dropedge" class="headerlink" title="Dropedge"></a>Dropedge</h2><p>这篇就是说Dropout效果这么好，我们也套用到GCN吧！</p>
<p>不过这里的dropout其实不同于CNN里面对feature的dropout（其实对feature的dropout在original GCN就用了），而是对weight的dropout，即每一层有概率的remove edge，（脑补一下，其实就相当于把GCN的weight全为1 改成了有些为0有些为1啊！）</p>
<h2 id="Position-aware-Graph-Neural-Networks"><a href="#Position-aware-Graph-Neural-Networks" class="headerlink" title="Position-aware Graph Neural Networks"></a>Position-aware Graph Neural Networks</h2><p>这个不是借用CNN的一些technique的理念，说下大概思路吧。</p>
<ul>
<li><p>问题：GCN只能捕捉局部的structure 信息，比如：</p>
<p><img src="/figures/image-20200627214400158.png" alt="image-20200627214400158"></p>
<p>不管怎样$v_1,v_2$（structurally isomorphic nodes）都是分配同样的label。那么怎么解决这个问题呢？</p>
<blockquote>
<p> node position can be captured by a low-distortion embedding by quantifying the distance between a given node and a set of anchor nodes</p>
</blockquote>
</li>
<li><p>先说下workflow吧，</p>
<ul>
<li>给$clog^2n$个子集$S_{i,j}，i&#x3D;1…\log n,j&#x3D;1,…,c\log n$，每个子集会随机选择所有点，每个点被选中的概率为$1&#x2F;2^i$,所以其实子集的大小其实是指数型增长的</li>
<li>PGNN的每一层operation如下，<ul>
<li>对于每个点，和每个子集里面的所有点（假设n个点）的featrue(假设d维) concat 起来-&gt;$n\times 2d$，如果子集里的点是距离为$k$外的（太远了），则return  zero值（而不是concat feature）。接着把concat结果aggreate起来-&gt; $2d$ （注意这里的aggregate也需要满足order invarianty！所以也只能用mean，max，sum等，作者直接用的mean）</li>
<li>对于不同的子集的结果再用aggregate $c\log^2n \times 2d \rightarrow 2d$。</li>
</ul>
</li>
</ul>
</li>
<li><p>总结：这篇的theorem用了Bourgain Theorem去证明只要$c\log ^2 n$个就子集就可以preserve the distances in the original graph with low distortion。。。不过很多paper里面的theorem都是比较巧妙的，让人觉着遥不可及。但是这个work解决GCN只有local structrue的大概方法也是类似的：就是想方设法的介绍一些global information进去，这里是通过介绍子集的方法，相当于给子集中的每个点多了一个额外信息：属于哪个子集。打个比方，假设所有人只有性别这一个label，我和坤坤都只有三个男性朋友，一个女性朋友，这样一层GCN就无法通过信息传输判断我的label了，但是如果创造2个子集（身份），分别是 爱rap的同学 和 爱打篮球的同学，然后发现我有一个爱rap的同学，2个爱打篮球的同学，坤坤有3个爱rap的同学，这样我和坤坤的label就是不一样的了。</p>
</li>
</ul>
<h2 id="COLORING-GRAPH-NEURAL-NETWORKS-FOR-NODE-DISAMBIGUATION"><a href="#COLORING-GRAPH-NEURAL-NETWORKS-FOR-NODE-DISAMBIGUATION" class="headerlink" title="COLORING GRAPH NEURAL NETWORKS FOR NODE DISAMBIGUATION"></a>COLORING GRAPH NEURAL NETWORKS FOR NODE DISAMBIGUATION</h2><p>引入了universal representations &amp; separability的概念，claim MPNN不是一个universal representation。解决方法（甚至出发点）和上篇paper有点相似。大概就是给node attribute 相同的点 从k种颜色中随机选一个颜色标上，k为hyper parameter，这样就实现了universal approximation。有两个concern</p>
<ul>
<li>若k不为 graph size （即所有点颜色都不一样），那么这种coloring sheme不是permutation invariance。这也是ICLR被拒的原因。</li>
<li>k&#x3D;1时，不就是所有点assign一个相同的颜色么。。没看懂。。。</li>
</ul>
<h2 id="GeomGCN"><a href="#GeomGCN" class="headerlink" title="GeomGCN"></a>GeomGCN</h2><p>同样，这个也不是借用CNN，说下大概思路吧。</p>
<ul>
<li><p>问题：1. 就像GIN说的（就这篇下面👇），现在基于聚合的都是垃圾，因为会将部分Non-isomorphic的图也给出同样的结果，如</p>
<p><img src="/figures/v2-7a1237527813b338926f6632bea84720_b.jpg" alt="img"></p>
<ol start="2">
<li>同样的 有些graph里面可能有些子图其实是有相似结构的，那么为什么不利用起来呢？</li>
</ol>
</li>
<li><p>基于以上两点，可以大概构思怎么解决 1. 将不同的node 当作不同的neighbor来聚合 2. 给每个node 加上一个能表示局部结构的node （这样局部结构相同的两点其实就可以share同样的feature了）</p>
</li>
<li><p>所以大概workflow如下：</p>
<ul>
<li>给每个点计算latent space上的node embedding，注意此处的node embedding是仅仅包含local struture 信息的（原文用了一些GCN之前的获取node embedding的方法，真的很纳闷为什么不直接用SOTA的GCN来获取node embedding）</li>
<li>对于每个点centroid，它都有两种邻居：真实邻居，转化成latent space下的邻居（另一个时空下的恋爱🐶），然后将每个邻居按照与centroid在latent space中的位置关系分为不同的邻居（为了解决第一个问题）。这里的实验latent space是二维的，所以邻居只分了四种，左上右上左下右下。</li>
<li>这样对于每种邻居，我们就分了四种点，这样就有四个aggregator，最后四个aggregator结果concat起来就行（因为总得保证是固定个结果嘛）</li>
</ul>
</li>
</ul>
<h2 id="Generalization-and-Representational-Limits-of-Graph-Neural-Networks"><a href="#Generalization-and-Representational-Limits-of-Graph-Neural-Networks" class="headerlink" title="Generalization and Representational Limits of Graph Neural Networks"></a>Generalization and Representational Limits of Graph Neural Networks</h2><p>定义decide：一个GNN Q如果decide了一个图性质P，则说明图性质P不同的两个图，由Q产生出来的graph embedding也肯定不同。</p>
<p>这篇文章前半部分就是用一些例子证明了：</p>
<ul>
<li>（用一些反例）基于聚合的很多GNN不能decide很多很简单的性质。</li>
</ul>
<h2 id="Simple-and-Deep-Graph-Convolutional-Networks"><a href="#Simple-and-Deep-Graph-Convolutional-Networks" class="headerlink" title="Simple and Deep Graph Convolutional Networks"></a>Simple and Deep Graph Convolutional Networks</h2><p>Use two simple techniques to make GNN deep</p>
<ul>
<li><p>Partially aggregate the input feature $H^0$, instead only aggregating previous layers features from its neighborhood</p>
</li>
<li><p>Adding a partial skip connection part</p>
</li>
<li><p><img src="/figures/image-20210311110145347.png" alt="image-20210311110145347"></p>
</li>
</ul>
<h2 id="PAIRNORM-TACKLING-OVERSMOOTHING-IN-GNNS"><a href="#PAIRNORM-TACKLING-OVERSMOOTHING-IN-GNNS" class="headerlink" title="PAIRNORM: TACKLING OVERSMOOTHING IN GNNS"></a>PAIRNORM: TACKLING OVERSMOOTHING IN GNNS</h2><p>Another paper to handle with oversoothing issue in GNNs, i.e., cannot generalize to deep network.</p>
<p><img src="/figures/image-20210311114447474.png" alt="image-20210311114447474"></p>
<p>Here, n is the number of nodes.</p>
<p>The normalization is simply used for the output of each layer</p>
<h2 id="Simplifying-Graph-Convolutional-Networks"><a href="#Simplifying-Graph-Convolutional-Networks" class="headerlink" title="Simplifying Graph Convolutional Networks"></a>Simplifying Graph Convolutional Networks</h2><p>Non-linear 在很多任务中没什么用，GNN可以直接用一个很简单的linear model来表示：</p>
<p>$Y &#x3D; softmax(S^KXW)$, where $S &#x3D; normalized \ A$</p>
<h1 id="理论层面"><a href="#理论层面" class="headerlink" title="理论层面"></a>理论层面</h1><h2 id="GIN"><a href="#GIN" class="headerlink" title="GIN"></a>GIN</h2><p>这个算是个人觉得最有意思的一篇了，大概就是说在座各位基于这种聚合的都是垃圾，最多也只能和从前的WL Test一样。<a href="https://wadmes.github.io/2019/09/14/GNN-GIN-GMN-study/">https://wadmes.github.io/2019/09/14/GNN-GIN-GMN-study/</a></p>
<h2 id="Weisfeiler-and-Leman-Go-Neural-Higher-order-Graph-Neural-Networks"><a href="#Weisfeiler-and-Leman-Go-Neural-Higher-order-Graph-Neural-Networks" class="headerlink" title="Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks"></a>Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</h2><p>A very good paper that proves a similar conclusion with GIN. (the difference is that they use linear transformation instead of MLP, and finally claim that double the layers (which becomes MLP actually) can have the same performance with WL test)</p>
<p>And then propose a k-GNN based on k-dimensional WL test. Basically, the “supernodes” is all k tuples of the graph.</p>
<h2 id="THE-LOGICAL-EXPRESSIVENESS-OF-GRAPH-NEURAL-NETWORKS"><a href="#THE-LOGICAL-EXPRESSIVENESS-OF-GRAPH-NEURAL-NETWORKS" class="headerlink" title="THE LOGICAL EXPRESSIVENESS OF GRAPH NEURAL NETWORKS"></a>THE LOGICAL EXPRESSIVENESS OF GRAPH NEURAL NETWORKS</h2><p>这篇（ICLR2020）大概就是接着上一篇GIN（ICLR2019）说，诶，确实以前的聚合（自己的+邻居的）都是垃圾，但我这个不一样了，对于每个点还有了个全局（readout）信息。</p>
<p>首先先从一个最简单的给点二分类的问题说起：如果要判断一个点是不是isolated node。我们是不是就没法正确分类了？因为true的点肯定没法传输信息。这个例子实在是妙啊。。</p>
<p>那么这个全局信息是什么呢？就是所有node feature的sum啦。。（ps。。头皮发麻）</p>
<h2 id="WHAT-GRAPH-NEURAL-NETWORKS-CANNOT-LEARN-DEPTH-VS-WIDTH"><a href="#WHAT-GRAPH-NEURAL-NETWORKS-CANNOT-LEARN-DEPTH-VS-WIDTH" class="headerlink" title="WHAT GRAPH NEURAL NETWORKS CANNOT LEARN: DEPTH VS WIDTH"></a>WHAT GRAPH NEURAL NETWORKS CANNOT LEARN: DEPTH VS WIDTH</h2><p>这篇文章的作者。。emm。。怎么说呢，一看就不是国人同时又是为计算机基础知识特别好的大神。</p>
<p>文章的证明需要比较强的计算机背景，不过结论很简洁易懂：</p>
<blockquote>
<p>该研究证明，如果我们想让 GNN 为常见的图问题（如环检测、直径估计、顶点覆盖等）提供解决方案，则节点嵌入的维度（即网络宽度 w）与层数（即网络深度 d）的乘积应与图大小 n 成正比，即 dw &#x3D; O(n)。</p>
</blockquote>
<h2 id="Approximation-Ratios-of-Graph-Neural-Networks-for-Combinatorial-Problems"><a href="#Approximation-Ratios-of-Graph-Neural-Networks-for-Combinatorial-Problems" class="headerlink" title="Approximation Ratios of Graph Neural Networks for Combinatorial Problems"></a>Approximation Ratios of Graph Neural Networks for Combinatorial Problems</h2><p>这篇将GNN和distributed local algorithms 结合了起来，先证明了GNN和distributed local algorithms 是等价的，然后利用以前work对distributed local algorithms的研究得出的结论直接给出了GNN的一些结论。</p>
<p>大概就是现在两种GNN， MB-GNN 和 SB-GNN都比不上 VVC-GNN，定义分别是：</p>
<p><img src="/figures/image-20200819215408321.png" alt="image-20200819215408321"></p>
<p><img src="/figures/image-20200819215427294.png" alt="image-20200819215427294"></p>
<p><img src="/figures/image-20200819215438565.png"></p>
<p>因此，提出了一个VVC-GNN。大概意思就是给别一个边的两个出口都assign一个port，使得所有点给它邻点的信息完全不一样。</p>
<p>最后实验就是用这个model + reinforcement learning 证明了 这个model可以解决很简单的组合优化问题，但是MB，SB都不可以。</p>
<p>在我看来，有个点是没解决的：</p>
<ul>
<li>这种方法注定导致不能迁移到其他graph（相当于给每个edge的port一个人为的index）</li>
<li>这个方法训练时间需要很久，但同时不能用到其他graph。。也太扯了吧。</li>
</ul>
<p>关于distributed local algorithms的定义，请参考 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1205.2051.pdf">https://arxiv.org/pdf/1205.2051.pdf</a></p>
<h2 id="Distance-Encoding-–-Design-Provably-More-Powerful-Graph-Neural-Networks-for-Structural-Representation-Learning"><a href="#Distance-Encoding-–-Design-Provably-More-Powerful-Graph-Neural-Networks-for-Structural-Representation-Learning" class="headerlink" title="Distance Encoding – Design Provably More Powerful Graph Neural Networks for Structural Representation Learning"></a>Distance Encoding – Design Provably More Powerful Graph Neural Networks for Structural Representation Learning</h2><p>添加了一个distance encoding to encode certain distance from S to a node u， 其中S是图中的一部分structure，想学的target structure feature就是S的feature，若S为全图，则为全图的feature(graph embedding). </p>
<p>这里的distance encoding 有比较复杂的formal形式，但是实验里面用的其实就是最简单的两点之间最短距离。</p>
<h2 id="Design-Space-for-Graph-Neural-Networks"><a href="#Design-Space-for-Graph-Neural-Networks" class="headerlink" title="Design Space for Graph Neural Networks"></a>Design Space for Graph Neural Networks</h2><p><strong>Motivation:</strong></p>
<ul>
<li>Surging new GNN tasks, with different requirements</li>
<li>Large design space of GNNs</li>
</ul>
<p><strong>Design space:</strong></p>
<ul>
<li>BN; Number of Pre-process layer (MLP); Number of Message Passing Layers; Sum&#x2F;Mean&#x2F; and son on</li>
</ul>
<p><strong>Task space:</strong></p>
<ul>
<li>Synthetic tasks: node features&#x2F; labels&#x2F; node&#x2F;graph classification </li>
<li>Real-world tasks:  6 node classification benchmarks from [26, 28], and 6 graph classification tasks from [14].</li>
</ul>
<p><strong>GraphGym</strong></p>
<ul>
<li>Modularized GNN implementation</li>
<li>Standardized GNN evaluation.</li>
<li>Reproducible and scalable experiment management</li>
</ul>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results:"></a><strong>Results:</strong></h3><ul>
<li><strong>BN</strong> good!</li>
<li><strong>No Dropout</strong> good!</li>
<li><strong>PRELU</strong> good!</li>
<li><strong>SUM</strong> good!</li>
<li><strong>Adam</strong> good (compared to sgd)</li>
<li><strong>400 epoch</strong> good than 100,200</li>
</ul>
<p>However, besides these “common-sense” conclusions, <strong>desirable design choices for Aggregation, Message passing layers, layer connectivity and post-processing layers drastically vary across tasks</strong></p>
<p><strong>Tasks can be roughly clustered into two groups:</strong></p>
<ul>
<li>(1) node classification over real-world graphs; with <strong>rich node features</strong>; GNN designs that can better propagate <strong>feature information</strong> are preferred; </li>
<li>(2) node classification over synthetic graphs and all graph classification tasks; require <strong>graph structural information</strong></li>
</ul>
<p>How to determine the “best” design for a new task?</p>
<ul>
<li>using $M&#x3D;12$ echor models to test the task, </li>
<li>find the most similar task based on the rankings of the echor models. </li>
<li>use the best design of the most similar task as the “best” design</li>
</ul>
<h2 id="Identity-aware-Graph-Neural-Networks"><a href="#Identity-aware-Graph-Neural-Networks" class="headerlink" title="Identity-aware Graph Neural Networks"></a>Identity-aware Graph Neural Networks</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ul>
<li>MP GNN cannot distinguish nodes with the same computational graph.</li>
<li>Although  task-specific feature augmentation;  is not generic and hamper the inductive power </li>
<li>Previous works  task specific; and not based on MP GNN-&gt;not efficient</li>
</ul>
<p><img src="/figures/image-20210306233434885.png" alt="image-20210306233434885"></p>
<h2 id="DropGNN-Random-Dropouts-Increase-the-Expressiveness-of-Graph-Neural-Networks"><a href="#DropGNN-Random-Dropouts-Increase-the-Expressiveness-of-Graph-Neural-Networks" class="headerlink" title="DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks"></a>DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks</h2><p>非常漂亮的文章！</p>
<p>大概就是GNN的upper bound是WL，那么有没有可能突破这个upper bound呢？方法是简单的randomly drop node，这样一个graph 就对应成了 一堆graph 的distribution，那么不同graph 的distribution大概率是不一样的！</p>
<p>这个问题是相同graph的distribution也可能不一样，除非randomly drop的次数足够多，就像投硬币一样。文章里面还给了solid proof about the probability of the distribution close to the real likelihood.</p>
<p>将statistics 里面的一些东西运用到graph里面来，感觉很有潜力啊！</p>
<h1 id="Some-other-interesting-papers"><a href="#Some-other-interesting-papers" class="headerlink" title="Some other interesting papers"></a>Some other interesting papers</h1><h2 id="NEURAL-SUBGRAPH-MATCHING"><a href="#NEURAL-SUBGRAPH-MATCHING" class="headerlink" title="NEURAL SUBGRAPH MATCHING"></a>NEURAL SUBGRAPH MATCHING</h2><p>Done by Stanford Group. The problem is to determine the presence of a given query graph in a large target graph.The key insight that makes NeuroMatch work is to define <strong>an embedding space where subgraph relations are preserved</strong>. That is, : If $G_v$ (k-hop neighborhood of node v) is a subgraph of $G_u$, then node v should be embedded to the lower-left of u. Such relationship is preserved in the embedding space by an elegantly-designed loss function:</p>
<p><img src="/figures/image-20210306233700395.png" alt="image-20210306233700395"></p>
<h2 id="Beyond-Homophily-in-Graph-Neural-Networks-Current-Limitations-and-Effective-Designs"><a href="#Beyond-Homophily-in-Graph-Neural-Networks-Current-Limitations-and-Effective-Designs" class="headerlink" title="Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"></a>Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</h2><p>This paper discusses GNNs in graphs where connected nodes may have <em>different</em> class labels and <em>dissimilar</em> features. </p>
<p>Three techniques are used:</p>
<ol>
<li><strong>concatenate</strong> aggreated features insteaed of sum or mean them</li>
<li>Higher-order Neighborhoods, concatenate two-order neighborhood</li>
<li>concatenate (at the final layer) all learned ego-representations at previous layers</li>
</ol>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>写完后，忽然发现了一个也可以算巧合的规律吧？所有方法层面的work基本都是国人lead，但是理论层面的work却寥寥无几。这算不算是一种悲哀呢？方法层面的东西固然重要、效果好，容易发paper，但是理论层面的才是有可能真正推动时代进步的那个钥匙，不过似乎大家都对这种“不好发paper”、“没有确定性”、“难入门”的东西有一种共识性的排斥，算是一种悲哀吧！</p>
<p>其实自己何尝不是这样呢。做了五六个work，除了openmpl，其他的基本都是比较讨巧，迎着市场需求（学术圈热点）做的东西。这是这个时代的无奈，也是我个人的无奈吧，其实也是蛮想做一个沉下心学习研究一两年然后真正contribute to the development of CS的东西，可是现在还只是一个再过几个月要靠publication去砸PHD commitee脸的小小mphil；就算phd了也只是一个担心qualification、担心graduation的PHD student&#x2F;candidate；就算当教授了也需要paper去争tenure啊；就算当了professor，也得对学生负责，在他没有成果前不可能让他做一些我自己都无法保证的东西吧？好像真正可以肆无忌惮做这种work的时间段只有achievement差不多时候的PHD year4&#x2F;5了？</p>
<p>扯的有点多，说回GCN吧。总的看来，其实各种technique&#x2F;theory还是在发展中的，不过里面确实还有很多可以发掘的东西，而且从功利化发paper的角度，不仅仅是Common GCN, 很多子分支，譬如point cloud，譬如heteogeneous graph（参考[<a href="https://wadmes.github.io/2019/12/17/heterogenous%20network/]">https://wadmes.github.io/2019/12/17/heterogenous%20network/]</a>(<a href="https://wadmes.github.io/2019/12/17/heterogenous">https://wadmes.github.io/2019/12/17/heterogenous</a> network&#x2F;)） 这些子分支都是水paper的好方向哪。。</p>
<p>另外就是虽然所有paper的conclusion或者method看起来很简单：这不是本科生都能想到的吗？但是里面的story writing绝对是我得好好研究的：怎么将一个简单的solution包装的很elegant，它解决什么问题的？有什么理论去支撑的？实验怎么设置？这里面都是学问啦。总之，虽然novelty不够，但是发现了这些work的一些共同点：</p>
<ul>
<li>大部分都有看起来牛逼哄哄的theorem&#x2F;proof（对不起因为大部分theorem我没有仔细看，所以只能暂时说看起来。。。）</li>
<li>results部分不会特别出彩，但肯定不是完全比不上别人。1% improvement 🐶，当然如果是专为解决特定问题的，譬如logical问题的那位，就是可以完爆了。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/06/17/How-to-use-grammarly-to-check-your-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/17/How-to-use-grammarly-to-check-your-paper/" class="post-title-link" itemprop="url">How to use grammarly to check your paper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-06-17 19:12:43 / Modified: 08:17:31" itemprop="dateCreated datePublished" datetime="2020-06-17T19:12:43-04:00">2020-06-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/06/17/How-to-use-grammarly-to-check-your-paper/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/17/How-to-use-grammarly-to-check-your-paper/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When we finish a paper draft by latex, we may find it difficult to directly use Grammarly to check the grammar since our paper is a .pdf format.</p>
<p>Here is a simple instruction to use Grammarly for your paper. If you have some other more elegant methods, feel free to leave a comment here.  :)</p>
<p>Here are the steps:</p>
<ul>
<li><p>Copy the text in the paper and paste it into VSCode</p>
</li>
<li><p>Replace all “\n” terms into space “ “ terms</p>
</li>
<li><p>Copy the processed text in VSCode and paste it into Grammarly</p>
</li>
<li><p>Find the error and fix it.</p>
<p>![截屏2020-06-17 下午7.19.53](&#x2F;figures&#x2F;截屏2020-06-17 下午7.19.53.png)</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/06/01/PyTorch-Study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/01/PyTorch-Study/" class="post-title-link" itemprop="url">ICCAD Proj 总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-06-01 14:33:06 / Modified: 05:02:31" itemprop="dateCreated datePublished" datetime="2020-06-01T14:33:06-04:00">2020-06-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/06/01/PyTorch-Study/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/01/PyTorch-Study/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>离ICCAD还有三星期的时候和老板说想放弃投ICCAD，最后在他的“威逼利诱”下，再加之自己的年轻气盛还是答应了并全力以赴的完成了。想想也是蛮了不起的，不过最后一两个星期实在是太累了点，希望以后还是少点这么肝的时候吧。</p>
<p>这大概一个月，一边做proj一边就有新的感触，每有一个感触，都会自己先记一笔，然后不知不觉的备忘录上就塞满了感触。。。现在一次性都记下来当作以后的吸取教训吧。</p>
<h2 id="Idea-amp-Paper-Review"><a href="#Idea-amp-Paper-Review" class="headerlink" title="Idea &amp; Paper Review"></a>Idea &amp; Paper Review</h2><p>前面两个proj 都分成了c++ 和python&#x2F;pytorch两个板块，因为比较尊重本科生的意见，所以都让他们先选负责的板块（所以都是选的python）。但是有个严重问题：pytorch部分往往和整个proj的idea相关，所以让本科生看paper 找idea实在是好刚没用在刀刃上。因此 两个proj都耽误了比较长的时间。好在自己看paper 总结idea的水平还是比较高了，后面都很快的调整好了方向。</p>
<p>以后在我看来本科生的帮助应该是：</p>
<ul>
<li>跑实验这种大家都需要时间成本，而且学习成本较低的work</li>
<li>画图，implementation就取决于本科生的学习能力和是否有过类似经验了。倘若我和ta都没学过，那让ta来学一下画图也可以（学习成本一个人花费就可以了）</li>
<li>整个idea方向，除非是比较厉害的本科生（这么看我当年其实还是蛮厉害的，在南科大work都是我来看paper想idea），否则应该是我来看paper 总结，想idea，同时简单的传达意思给ta。毕竟确实我看paper的经验，水平肯定高过大部分本科生的。</li>
</ul>
<p>最后再吐槽下吧，好羡慕南科大有那么多本科生帮忙，然后本科生也有各种1&#x2F;2&#x2F;3作paper，实在是双赢。CU的本科教育就花在了倾庄、人文教育上了（当然有research，但是比起内地学校有组织的research还是差远了）。</p>
<h2 id="Coding-style"><a href="#Coding-style" class="headerlink" title="Coding style"></a>Coding style</h2><p>这是看了DGCNN的mit学长的code发出的感慨。感觉任何语言，尤其python，必须要有一套自己习惯的coding style，包括但不限于：</p>
<ul>
<li>argments</li>
<li>i&#x2F;o</li>
<li>error exception handling</li>
<li>model architecture</li>
</ul>
<p>这样的话，自己implementation的时候会方便很多，同时本科小朋友帮忙跑实验也会简单很多（改参数就可以）。哦对了，还有一个要习惯用git。。</p>
<h2 id="Torch-Tensor-Operation"><a href="#Torch-Tensor-Operation" class="headerlink" title="Torch Tensor Operation"></a>Torch Tensor Operation</h2><p>这算是implementation时候的一个点吧。之前习惯了for loop，这次用多了pytorch tensor后发现实在是太爽了。不仅是速度提升（1000x），整个code 也elegant很多。说下从for loop 转成tensor operation的大概idea吧：</p>
<ul>
<li>如果index 不影响operation的话，可以直接用tensor[:]操作</li>
<li>如果影响就需要 生成一个index tensor <ul>
<li>灵活利用torch gather（有一串index 和一个index对应的value， 想通过index取出所有对应value）</li>
<li>利用torch where 来代替 if else</li>
</ul>
</li>
</ul>
<p>晒一下自己生成kbbox（对于每个点，return k个最近的且两点bbox间没有第三个点的所有neighbors）的elegant code吧（对比for loop):</p>
<p>For loop version:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> size <span class="keyword">in</span> <span class="built_in">range</span>(SIZE_OF_NET):</span><br><span class="line">    num_of_sample = <span class="built_in">int</span>(size)</span><br><span class="line">    data = train_data[size].cpu()</span><br><span class="line">    <span class="comment"># data = train_data[size]</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">len</span>(data) == <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    index = torch.zeros([data.size(<span class="number">0</span>),data.size(<span class="number">1</span>),num_of_sample]) <span class="comment">#B,N,K</span></span><br><span class="line">    <span class="keyword">for</span> N <span class="keyword">in</span> <span class="built_in">range</span>(data.size(<span class="number">1</span>)):</span><br><span class="line">        <span class="comment">#initialize all value as current node index N</span></span><br><span class="line">        index[:,N,:] = N</span><br><span class="line">    <span class="comment">#for each batch B</span></span><br><span class="line">    max_sample = <span class="number">0</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PROCESS &quot;</span>,size)</span><br><span class="line">    <span class="keyword">for</span> B <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)): </span><br><span class="line">        <span class="comment">#for each node N</span></span><br><span class="line">        <span class="keyword">if</span>(B % <span class="number">1000</span> == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;PROCESS &quot;</span>,size,B)</span><br><span class="line">        <span class="keyword">for</span> N <span class="keyword">in</span> <span class="built_in">range</span>(data.size(<span class="number">1</span>)):</span><br><span class="line">            sample = <span class="number">0</span></span><br><span class="line">            <span class="comment">#We then check each point Q: whether the bounding box between Q and K has no other points, if yes, we add Q in index[B,N,:]</span></span><br><span class="line">            <span class="keyword">for</span> Q <span class="keyword">in</span> <span class="built_in">range</span>(data.size(<span class="number">1</span>)):</span><br><span class="line">                <span class="keyword">if</span>(Q == N):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#Coordinate of Q related to N</span></span><br><span class="line">                C_QN = data[B,Q,:] - data[B,N,:]</span><br><span class="line">                is_index = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">for</span> J <span class="keyword">in</span> <span class="built_in">range</span>(data.size(<span class="number">1</span>)):</span><br><span class="line">                    <span class="keyword">if</span>(J == Q <span class="keyword">or</span> J == N):</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    C_JN = data[B,J,:] - data[B,N,:]</span><br><span class="line">                    <span class="keyword">if</span> ((C_JN*C_QN) &gt; (C_JN * C_JN)).<span class="built_in">min</span>():</span><br><span class="line">                        <span class="comment"># print(&quot;Q cannot be index of N due to J&quot;,C_QN,C_JN)</span></span><br><span class="line">                        is_index = <span class="literal">False</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span>(is_index):</span><br><span class="line">                    sample += <span class="number">1</span></span><br><span class="line">                    index[B,N,sample] = Q</span><br><span class="line">            <span class="keyword">if</span>(sample &gt; max_sample):</span><br><span class="line">                max_sample = sample</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PROCESS &quot;</span>,size, <span class="string">&quot;max_sample: &quot;</span>,max_sample)</span><br><span class="line">    torch.save(index[:,:,:max_sample+<span class="number">1</span>],<span class="string">&quot;../../data/tree/&quot;</span>+<span class="built_in">str</span>(size))</span><br></pre></td></tr></table></figure>

<p>Tensor operation version:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#data : raw distance data (B,N,2)</span></span><br><span class="line">data = tensor</span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">len</span>(data) == <span class="number">0</span>):</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="comment"># print(tensor.size())</span></span><br><span class="line">B,N,_ = data.size()</span><br><span class="line"><span class="comment"># data = train_data[size]</span></span><br><span class="line"><span class="comment"># print(tensor[0])</span></span><br><span class="line"><span class="comment">#inner : (B,N,2) * (B,2,N) = (B,N,N )</span></span><br><span class="line">inner = -<span class="number">2</span> * torch.matmul(data,data.transpose(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">xx = torch.<span class="built_in">sum</span>(data ** <span class="number">2</span>, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#pairwise_distance: relative distance between N1 &amp; N2 (B,N1,N2) </span></span><br><span class="line">pairwise_distance = -xx - inner - xx.transpose(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># print(&quot;pairwise_distance&quot;,pairwise_distance[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BNN2 = data.view(B,N,1,2).repeat(1,1,N,1)</span></span><br><span class="line"><span class="comment"># BQQ2 = data.view(B,1,N,2).repeat(1,N,1,1)</span></span><br><span class="line"><span class="comment"># print(&quot;BNN2&quot;,BNN2[0][0])</span></span><br><span class="line"><span class="comment"># print(&quot;BQQ2&quot;,BQQ2[0][0])</span></span><br><span class="line"><span class="comment">#BNQ2: value in (b,n,q) is the relative coordinates between node n and node q at b_th batch.</span></span><br><span class="line"><span class="comment"># BNQ2 = BNN2 - BQQ2</span></span><br><span class="line">BNQ2 = data.view(B,N,<span class="number">1</span>,<span class="number">2</span>) - data.view(B,<span class="number">1</span>,N,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># print(&quot;BNQ2&quot;,BNQ2[0][0])</span></span><br><span class="line"><span class="comment">#CQN: value in (b,n,q,j) is the relative coordinates between node n and node q at b_th batch.</span></span><br><span class="line"><span class="comment">#CJN: value in (b,n,q,j) is the relative coordinates between node n and node j at b_th batch.</span></span><br><span class="line"><span class="comment">#UPDATE: The view step here is not neccesary since pytorch will automatically scale the tensor shape</span></span><br><span class="line">CQN = BNQ2.view(B,N,N,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">CJN = BNQ2.view(B,N,<span class="number">1</span>,N,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># print(&quot;CQN&quot;,CQN[0][0])</span></span><br><span class="line"><span class="comment"># print(&quot;CJN&quot;,CJN[0][0])</span></span><br><span class="line"><span class="comment">#BNQJ: value in (b,n,q,j) indicates that when n is the centroid, whether j is in the bounding box of q and n (and such makes q not connected to j)</span></span><br><span class="line"><span class="comment">#True means the j influences q</span></span><br><span class="line">BNQJ = (CQN*CJN &gt; CJN*CJN).<span class="built_in">min</span>(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># print(&quot;BNQJ&quot;,BNQJ[0][0])</span></span><br><span class="line"><span class="comment">#BNQJ: value in (b,n,q) indicates that when n is the centroid, whether q cannot connect to n</span></span><br><span class="line"><span class="comment">#True mans q cannot conncet n</span></span><br><span class="line">BNQ = BNQJ.<span class="built_in">max</span>(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># print(&quot;BNQ&quot;,BNQ[0])</span></span><br><span class="line"><span class="comment">#For those q who cannot connect to n (which is exactly True in BNQ), we assign a large distance value(10000)</span></span><br><span class="line">_,index = ((pairwise_distance - (BNQ.long() * <span class="number">10000</span>))).topk(k=<span class="number">4</span>,dim = <span class="number">2</span>)</span><br><span class="line"><span class="comment"># print(&quot;index&quot;,index[0])</span></span><br><span class="line"><span class="comment"># exit()</span></span><br><span class="line">torch.save(index,<span class="string">&quot;../../data/tree/kbbox_&quot;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;_&#x27;</span>+<span class="built_in">str</span>(j))</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个work应该是自己这么多work里面最满意的一个了？虽然比较仓促，但是整个story是连贯的，对应方法也不是和一般dl-based work一样讲究一个玄学，基本都是有规律有原因能解释的。最后的结果也是比较满意（最后两三天的时候加上了FLUTE的结果实在是不太行，临时把FLUTE remove了，最后一天才算出结果。。也是没谁了）</p>
<p>这个要是结果ICCAD没中可就滑稽了。投paper这个东西就真的可以叫做玄学了。ß</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/05/20/ICCAD%E5%89%8D%E7%9A%84%E5%81%B7%E6%87%92%E6%9C%88%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/20/ICCAD%E5%89%8D%E7%9A%84%E5%81%B7%E6%87%92%E6%9C%88%E8%AE%B0/" class="post-title-link" itemprop="url">ICCAD前的偷懒月记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-20 00:04:08" itemprop="dateCreated datePublished" datetime="2020-05-20T00:04:08-04:00">2020-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-18 12:47:37" itemprop="dateModified" datetime="2024-01-18T12:47:37-05:00">2024-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/05/20/ICCAD%E5%89%8D%E7%9A%84%E5%81%B7%E6%87%92%E6%9C%88%E8%AE%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/20/ICCAD%E5%89%8D%E7%9A%84%E5%81%B7%E6%87%92%E6%9C%88%E8%AE%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          Forbidden area!
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/05/20/ICCAD%E5%89%8D%E7%9A%84%E5%81%B7%E6%87%92%E6%9C%88%E8%AE%B0/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/04/26/Transformer-Attention-is-all-you-need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/26/Transformer-Attention-is-all-you-need/" class="post-title-link" itemprop="url">Transformer: Attention is all you need</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-26 02:07:16 / Modified: 02:26:38" itemprop="dateCreated datePublished" datetime="2020-04-26T02:07:16-04:00">2020-04-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/04/26/Transformer-Attention-is-all-you-need/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/26/Transformer-Attention-is-all-you-need/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>建议配合 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av56239558?from=search&seid=14406218127146760248">https://www.bilibili.com/video/av56239558?from=search&amp;seid=14406218127146760248</a> 食用。</p>
<p>关于read paper：我个人觉得有两块要读（思考）：一块是算法上，有什么interesting或者amazing的算法值得学习。另一块就是经验、思想上，作者是如何提出这个方法的，这个方法背后蕴藏了什么思想&#x2F;经验。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>RNN不利于parallelization</li>
<li>Attention可以无视sequence距离挖掘信息。（我爱这个笑着眼睛像月牙弯弯每天都像阳光一样点亮我的你-&gt; key information：我爱你）</li>
<li>如今的attention（16年）仍然是和RNN一起出现(used in conjunction with recurrent network)</li>
<li>现在的work：要么不能并行。要么可以并行（例如CNN）却不能挖掘远处关联。（我爱月牙弯弯）</li>
<li>Tranformer：第一个完全利用self-attention计算input和output的representation。（不用RNN，CNN）</li>
</ul>
<p>Include 反义词： preclude 排除，杜绝：This inherently sequential nature precludes parallelization within training examples</p>
<p>albeit：尽管</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Overview:</p>
<p><img src="/figures/image-20200426111253973.png" alt="image-20200426111253973"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">At each step: (sequence to sequence should be generated step by step)</span><br><span class="line">	middle sequence z = Encoder(Input sequence x(我爱你))</span><br><span class="line">	output sequence y_t = Decoder(z,y_&#123;t-1&#125;)</span><br><span class="line">	注意：图中右下角的output为之前的输出。</span><br></pre></td></tr></table></figure>

<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>LayerNorm(x + multi-head(x))</li>
<li>LayerNorm(x + feedforward(x))</li>
<li>keys,values,querys: input x</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li><p>Addtional sublayer compared with Encoder: Masked attention layer(ensures that the predictions for position i can depend only on the known outputs at positions less than i.)</p>
</li>
<li><p>Keys, values: middle sequence from encoder</p>
</li>
<li><p>querys: Masked multi-head attention</p>
</li>
</ul>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>A <strong>mapping</strong> from a query and a set of key-value pairs to an output</p>
<p>output:  a <strong>weighted sum</strong> of the values, where the weight assigned to each value is computed by a compatibility function（适合性方程？总之就是计算相似度） of the query with the corresponding key.</p>
<p><img src="/figures/image-20200426115525168.png" alt="image-20200426115525168"></p>
<p>$Q: [seq,d_k], K:[seq,d_k],V:[seq,d_v]$</p>
<p>$seq$: sequence length</p>
<p>注意 原x dimension 为$d_{model}$， 经过linear projection转化为 $d_k,d_v$ dimension</p>
<h3 id="Multi-head"><a href="#Multi-head" class="headerlink" title="Multi-head"></a>Multi-head</h3><p>$x:[d_{model}] &#x3D;&gt; [h *d_v]&#x3D;&gt;[d_{model}] $</p>
<p>h：head数目。（multihead最后concat起来，所以是$h*d_v$）</p>
<p>再经过一次linear transform到$d_{model}$</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>常见的position方法就是one-hot encoding，但显然不能input dimension匹配，而且若用one-hot encoding 直接sum 未免显得太蠢。</p>
<p>Transformer提出了利用正余弦的方法：</p>
<p><img src="/figures/image-20200426140105603.png" alt="image-20200426140105603"></p>
<p>PE即为目标embedding。pos为word所处位置，2i即为 $d_{model}$中的维度之一。</p>
<ul>
<li>PE(:,x)即为$x_{th}$ dimensition下所有position的维度。 可以看到是一个周期函数</li>
<li>PE(x,:)即为$x_{th}$ position下的embedding，是一个从波长为2pi 到 10000 * 2pi的函数。</li>
</ul>
<p>但这里反直觉的是 PE是和input直接相加。而不是concat。。。 奇怪呢。</p>
<p>Dropout -&gt; Residual -&gt; Layer Normalization (Some papers claim that layer normalization should be in the sub-layer instead of behind residual summation)</p>
<h2 id="Why-self-attention"><a href="#Why-self-attention" class="headerlink" title="Why self-attention"></a>Why self-attention</h2><ul>
<li># of operations</li>
<li>Parallel</li>
<li>path length between long-range dependencies in the network</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><ul>
<li>Dropout 效果明显</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/04/25/%E5%9B%9B%E6%9C%88%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/25/%E5%9B%9B%E6%9C%88%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" class="post-title-link" itemprop="url">四月的碎碎念</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-25 00:40:49" itemprop="dateCreated datePublished" datetime="2020-04-25T00:40:49-04:00">2020-04-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-18 12:18:58" itemprop="dateModified" datetime="2024-01-18T12:18:58-05:00">2024-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/04/25/%E5%9B%9B%E6%9C%88%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/25/%E5%9B%9B%E6%9C%88%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          Forbidden area!
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/04/25/%E5%9B%9B%E6%9C%88%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/04/06/%E8%AF%B7%E5%9B%9E%E7%AD%941988/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/06/%E8%AF%B7%E5%9B%9E%E7%AD%941988/" class="post-title-link" itemprop="url">请回答1988</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-06 23:36:20" itemprop="dateCreated datePublished" datetime="2020-04-06T23:36:20-04:00">2020-04-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 23:08:52" itemprop="dateModified" datetime="2020-04-09T23:08:52-04:00">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/life/" itemprop="url" rel="index"><span itemprop="name">life</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/04/06/%E8%AF%B7%E5%9B%9E%E7%AD%941988/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/06/%E8%AF%B7%E5%9B%9E%E7%AD%941988/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我是个很少看剧的人。在我看来，冗长的剧情、尤其是毫无营养又冗长的剧情、尤其是毫无营养又冗长的宫廷剧实在是浪费生命。不过我为数不多的几次看剧都是真香结尾：大学的射雕英雄传，信号真香了，这次的1988也真香了。用真香这个字眼其实也不太准确：我还是对豆瓣高评分的事物抱有期待的，毕竟自己也只是个喜好和大众口味很匹配的芸芸众生之一。</p>
<p>照例。写一些自己印象深刻的点。</p>
<h2 id="乌邦托"><a href="#乌邦托" class="headerlink" title="乌邦托"></a>乌邦托</h2><p>胡同里的所有人都太温暖了，温暖到我觉得这只可能是个乌邦托。德善很想被疼爱，但是只有两个荷包蛋时还是说自己不用，虽然自己其实也很委屈。爸爸知道了德善的委屈后，也可以如朋友般的和德善坐着聊天：”德善啊，对不起，爸爸也是第一次做爸爸。。“。善宇怕妈妈担心，把妈妈做的盒饭在进门前全吃完了。阿泽在知道爸爸的想法后，坦率的支持爸爸：“我只希望爸爸能过得幸福，毕竟爸爸也有自己的人生。”。正焕知道妈妈的情绪后，故意搞砸家里的事情。。。</p>
<p>虽然只是乌邦托，但是发现，做一个温暖的人、细心的人是真的很好哪。这些品质可以不被外界事物影响，留在自己身上一辈子，并且可以温暖到周边的人，这不是很幸福的事吗？其实说起来，从小寄人篱下的我从前倒挺会察言观色的，说好听点就是细心体贴，不过初三过后性格实在是变得太多太大条了，开朗如娃娃鱼可以，但是娃娃鱼好歹还是个人生导师、情商拉满呢，哪像我技术也不行，情商也不够，细心也丢了。</p>
<h2 id="喜欢"><a href="#喜欢" class="headerlink" title="喜欢"></a>喜欢</h2><p>不知道是逛虎扑知乎微博逛多了，还是自己变得现实了。</p>
<p>看完剧，想想自己。好像丧失了十几岁的那种喜欢，那种只想无条件对喜欢人好的喜欢，现在的自己变得很担心自己陷进爱情里，担心自己对她太好就是“舔狗”。其实想想，无条件对喜欢的人好就会是丧失自我的舔狗吗？恰恰相反，因为这些想法而丢了纯真的喜欢才是真的丧失自我，不是女方的舔狗，是“世俗”的舔狗。</p>
<p>什么是少年感？少年感是阿泽跟着去卫生间，还假装抽烟照顾德善的情绪。是正焕看着电影义无反顾的开车回去。是善宇的喜欢就是喜欢。很多时候，我们做出的选择是基于利益判断的，能赚钱是正收益，能提高ta对我的好感是正收益，如果付出的抵不上回报带来的数学期望，我们就不做了。但若尚为少年，怎么会考虑这么多东西？意气风发的少年是纯粹简单的。我喜欢你就是喜欢你，就是和你在一起就会很高兴，你不在就感觉要死了。所以会对你好，会吃醋，会关心你的动态，会照顾你的情绪。这些都是因为我喜欢你，也只是因为我喜欢你，如若有其他的原因，譬如希望在一起，那也变味了。</p>
<p>另外，还有一个也许看了这部剧的所有人都会同意的想法：爱就要大胆表现出来，命运不仅仅是偶然，是许多次要做出选择的瞬间组成的，让人讨厌的不是红绿灯，也不是时机，而是我的无数次犹豫不决。其实不仅仅是爱情，还有亲情，友情，许多东西都是如此，我爱你，所以我会表现出来，表现在方方面面，就像春天到来，你能闻到花香、听到鸟鸣，感受到舒服的阳光一样。</p>
<h2 id="阿泽"><a href="#阿泽" class="headerlink" title="阿泽"></a>阿泽</h2><p>比起简直就是初中前我的翻版的善宇，阿泽就是我的白月光啊！那种特别喜欢的人，那种特别想成为的人。温润如玉波澜不惊的性格真是太对我胃口了（我也不知道我现在怎么就是娃娃鱼的性格）。再想想自己高三努力的样子，发现我这个人真是个一辈子下来可以变化很大的角色。</p>
<p>像之前提到的，他很温暖，会照顾他人的情绪，娃娃鱼生气只有自己不知道善宇谈恋爱的事时立刻跳出来说自己也不知道（虽然撒谎很拙劣）。他对于自己的喜欢很坦率简单，我喜欢德善，喜欢女人的那种喜欢。他也很真实，放弃了自己喜欢的女孩后，会流泪难过，知道对方的心意后，又很直接的吻了上去，而不是像成年人一般有这样那样现实上的顾虑：那样多累啊？他还有我最想有的一点品质：知世故而不世故。采访了很长后，没有生气，而是说很晚了，让社长和他们一起吃晚饭。</p>
<p>就像弹幕说正峰的一样，阿泽也是“吾辈楷模”啊。希望自己以后，也可以成为一个如他般温柔细腻、知世故而不世故，一眼看过去就是干净简单的人。</p>
<h2 id="自己的回忆"><a href="#自己的回忆" class="headerlink" title="自己的回忆"></a>自己的回忆</h2><p>好像每个人都能从这部剧里面找到代入感一样，很多男生是正焕，是啊，青春期的男生，有几个会如阿泽善宇般直率的表白自己的心意呢。而我，肯定就是善宇了。所以从一开始就对这个男生有着先天的好感，懂事的令人心疼、会小心翼翼的照顾妈妈的想法，成绩也挺好，看着他穿白大褂的样子真有点恍惚：就好像我是医生一般。</p>
<p>同样也有一个院子长大的玩伴，不过自己不会表达想法的缺陷也让整个回忆没有剧里那么多友情有关的故事。有次，我和老辉看到肖华正在哭着拦着正扭打在一起的父母，我们两个只是苦笑：没有后续的行动了。我们两个也都有过类似的经历，所以苦笑，我们两个都不是那么温暖照亮他人的人，所以没有后续了。我想，如果是剧里面的任何一个少年，都会去安慰他吧。</p>
<p>剧看完了，其实更多的还是恍惚、难过、不舍。不仅仅是对剧，还有对剧里所代表的那段时光和感情。我已经不是中学生，甚至大学生都算不上了，学生时代的爱情和我估计也是没什么缘分了，和家人邻里也不可能有长时间的相处了，朋友好像也只有博一一个可以称为挚友。</p>
<p>不过也不算太糟：学生的爱情不可能了，但我还可以如少年般的去喜欢一个人。家人邻里不能长时间相处，但我也还可以做一个温暖的人，朋友有博一还不香吗，博一这样简单的人世上能碰到便算作有幸了呢！</p>
<p>差不多也到结尾了，写完这一通，好像也活得明白通透了一点，希望自己如自己所愿，做一个简单的少年。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Li Wei"
      src="/images/liwei3.jpg">
  <p class="site-author-name" itemprop="name">Li Wei</p>
  <div class="site-description" itemprop="description">人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">148</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wadmes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wadmes" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:werry715@gmail.com" title="E-Mail → mailto:werry715@gmail.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://music.163.com/#/playlist?id=49000450" title="Music → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;playlist?id&#x3D;49000450" rel="noopener" target="_blank"><i class="fas fa-music fa-fw"></i>Music</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/wadmes/" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;wadmes&#x2F;" rel="noopener" target="_blank"><i class="fab fa-steam fa-fw"></i>Steam</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="link fa-fw"></i>
      Interesting links!
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://wadmes.github.io/2019/12/11/EDA-family-tree/" title="https:&#x2F;&#x2F;wadmes.github.io&#x2F;2019&#x2F;12&#x2F;11&#x2F;EDA-family-tree&#x2F;">EDA family tree</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://wenku.baidu.com/view/f5cdb37aeff9aef8951e0673" title="https:&#x2F;&#x2F;wenku.baidu.com&#x2F;view&#x2F;f5cdb37aeff9aef8951e0673" rel="noopener" target="_blank">小爷当年的高考满分作文</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Wei</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='50' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'NmNqnbed4p8koRJWwGm6SdrW-gzGzoHsz',
      appKey     : 'ECnYjjxnwouXPYlJjrjLJhv9',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
